{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8529b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "\n",
    "# Modelling\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77416fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric\n",
    "# rmse = mean_squared_error(y_actual, y_predicted, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb935ebc",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65dca9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files to read in\n",
    "gcse_files = glob.glob(\"../fake_data/synthetic_*_gcse_20[1-2][0, 8-9].csv\")\n",
    "npd_files = glob.glob(\"../fake_data/synthetic_npd_ks4_student_20[1-2][0, 8-9].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c5d16",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9017af",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_grades(data = pd.DataFrame, grade_col = str):\n",
    "    \n",
    "    # Drop rows with missing grades\n",
    "    data = data.dropna(subset = grade_col)\n",
    "    # Convert U grade to 0\n",
    "    data.loc[data[grade_col] == \"U\", grade_col] = \"0\"\n",
    "    # Convert grades to numeric from string format\n",
    "    data = data[data[grade_col].isin([str(x) for x in (range(0, 10))])]\n",
    "    data[grade_col] = data[grade_col].astype(float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0284b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_gcse_data(df = pd.DataFrame):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes raw GCSE exam data (2017-2020 files), filters it\n",
    "    appropriately and processes it. \n",
    "    Returns a DataFrame with a reduced number of columns.\n",
    "    Full steps taken can be seen in code commenting or in\n",
    "    Methodology section of capstone.\n",
    "    --------------------------------------------------\n",
    "    df = DataFrame of raw GCSE data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy to prevent in-place changes\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Make cols lowercase\n",
    "    data.columns = [x.lower() for x in data.columns]\n",
    "    \n",
    "    # Reformat examseries to year col\n",
    "    data[\"year\"] = data.examseries.apply(lambda x: x.split()[1])\n",
    "    \n",
    "    # Remove candidates who were not 16 on 31st August\n",
    "    data = data.query(\"yearendage == 16\")\n",
    "    # Remove private candidates\n",
    "    data = data.query(\"privatecandidate == False\")\n",
    "    # Commented out below since all True in synthetic data\n",
    "    # Remove partial absentees\n",
    "#     data = data.query(\"partialabsence == False\")\n",
    "    # Remove candidates without prior attainment or that weren't matched in NPD\n",
    "    data = data.dropna(subset = [\"normalisedks2score\", \"npdmatchround\"])\n",
    "    \n",
    "    # Remove candidates with 0 prior attainment (errors in data)\n",
    "    data = data[data.normalisedks2score > 0]\n",
    "    \n",
    "    # Remove non-reformed GCSEs\n",
    "    data = data[data.reformphase.isin(['Ofqual-regulated Phase 1 reformed GCSE FC',\n",
    "                                       'Ofqual-regulated Phase 2 reformed GCSE FC'])]\n",
    "    # Recode tier into foundation or not foundation\n",
    "    data.loc[data.tier != \"F\", \"tier\"] = \"Not F\"\n",
    "    \n",
    "    # Process grade column inplace\n",
    "    data = process_grades(data, grade_col = \"grade\")\n",
    "    \n",
    "    # Standardise the KS2 prior attainment to between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    data.normalisedks2score = scaler.fit_transform(data[['normalisedks2score']])\n",
    "    \n",
    "    # Get candidates who took at least 8 GCSEs\n",
    "    grouped = data.groupby(\"uidp\").count()\n",
    "    at_least_8 = set(grouped[grouped.examseries >= 8].index.to_list())\n",
    "    # Get candidates who took English and Maths\n",
    "    eng_math = set(data[data.jcqtitle.isin([\"Mathematics\", \"English language\"])].uidp)\n",
    "    # Get candidates who took English and Maths and >= 8 GCSEs\n",
    "    filtered_ids = at_least_8 & eng_math\n",
    "    # Beware that since this is simulated data, it's wrong\n",
    "    filtered = data[data.uidp.isin(filtered_ids)]\n",
    "    \n",
    "    # Select cols needed for modelling and dropnas\n",
    "    gcse_cols = [\"uidp\", \"year\", \"jcqtitle\", \"tier\", \"centretypedesc\",\n",
    "                 \"normalisedks2score\", \"grade\", \"centreassessmentgrade\"]\n",
    "    filtered = filtered[gcse_cols]\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845ad2f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load and process all the GCSE exam data\n",
    "gcse_data = pd.DataFrame()\n",
    "# Iterate through files\n",
    "for file in gcse_files:\n",
    "    # Perform filtering/pre-processing\n",
    "    year_df = process_gcse_data(pd.read_csv(file))\n",
    "    # Process the CAG column too\n",
    "    if \"2020\" in file:\n",
    "        year_df = process_grades(year_df, \"centreassessmentgrade\")\n",
    "    # Create dummy value for other years\n",
    "    else:\n",
    "        year_df.centreassessmentgrade = np.NaN\n",
    "        \n",
    "    # Merge with other years\n",
    "    gcse_data = pd.concat([gcse_data, year_df])\n",
    "    # Delete var to save memory\n",
    "    del year_df\n",
    "# Reset index\n",
    "gcse_data = gcse_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560cc389",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NPD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a21cd9b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_npd(data = pd.DataFrame):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes raw NPD data (2017-2020 files), filters it\n",
    "    appropriately and processes it. \n",
    "    Returns a DataFrame with a reduced number of columns.\n",
    "    Full steps taken can be seen in code commenting or in\n",
    "    Methodology section of capstone.\n",
    "    --------------------------------------------------\n",
    "    df = DataFrame of raw NPD data\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Copy to prevent inplace changes\n",
    "    df = data.copy()\n",
    "    # Make cols lowercase\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    # Select the columns that are common across files\n",
    "    npd_cols = [\"uidp\", \"ks4_ealgrp_ptq_ee\", \"ks4_gender\"]\n",
    "    # Get the bases for the columns that change in suffix in each file\n",
    "    col_bases = [\"ethnicgroupmajor\", \"fsmeligible\", \"senprovisionmajor\"]\n",
    "    # Get the suffix part that changes\n",
    "    year_ending = int(file[-6:-4])\n",
    "    # Dynamically select those cols with changing suffixes\n",
    "    npd_cols.extend([col_base + f\"_spr{year_ending}\" for col_base in col_bases])\n",
    "    # Also add in most recent IDACI score\n",
    "    npd_cols.append(sorted([x for x in df.columns if \"idaciscore\" in x])[-1])\n",
    "    \n",
    "    # Select the needed columns\n",
    "    df = df[npd_cols]\n",
    "    # Add in year col\n",
    "    df[\"year\"] = f\"20{year_ending}\"\n",
    "    # Rename columns\n",
    "    clean_cols = [\"uidp\", \"eal\", \"gender\", \"ethnicity\",\n",
    "              \"fsm\", \"sen\", \"idaci\", \"year\"]\n",
    "    df.columns = clean_cols\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37971c9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "col_dict = dict()\n",
    "for file in npd_files:\n",
    "    col_dict[file[-8:-4]] = pd.read_csv(file).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ddaceb6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set(col_dict[\"2020\"]) & set(col_dict[\"2019\"]) & set(col_dict[\"2018\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "946acd48",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set(col_dict[\"2020\"]) - set(col_dict[\"2019\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df94cd91",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create df to store each year's data in\n",
    "npd_data = pd.DataFrame()\n",
    "\n",
    "# Iterate through files\n",
    "for file in npd_files:\n",
    "    # Load data\n",
    "    df = pd.read_csv(file)\n",
    "    # Process the NPD data\n",
    "    df = process_npd(df)\n",
    "    # Combine into dataframe\n",
    "    npd_data = pd.concat([npd_data, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ebe8c",
   "metadata": {},
   "source": [
    "# Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e38a67",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def recode_cols(data = pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Takes processed merged GCSE exam and NPD data (2017-2020 files),\n",
    "    filters it appropriately and processes it. \n",
    "    It recodes several columns into fewer numbers of categories\n",
    "    to make modelling easier.\n",
    "    Returns a DataFrame with a reduced number of columns.\n",
    "    Full steps taken can be seen in code commenting or in\n",
    "    Methodology section of capstone.\n",
    "    --------------------------------------------------\n",
    "    df = DataFrame of merged NPD/GCSE data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy to prevent inplace changes\n",
    "    df = data.copy()\n",
    "    # Filter EAL to remove NAs or unclassifieds\n",
    "    df = df[df.eal.isin([1,2])]\n",
    "    # Filter ethnicity to remove unclassifieds/NaNs\n",
    "    df = df[df.ethnicity.isin([\"AOEG\", \"ASIA\", \"BLAC\", \"CHIN\",\n",
    "                          \"MIXD\", \"WHIT\"])]\n",
    "    # Filter and recode SEN to remove unclassifieds and make SEN/not SEN\n",
    "    df = df[df.sen.isin([\"1_NON\", \"2_SNS\", \"3_SS\"])]\n",
    "    df.loc[df.sen != \"1_NON\", \"sen\"] = \"SEN\"\n",
    "    df.loc[df.sen == \"1_NON\", \"sen\"] = \"No SEN\"\n",
    "    \n",
    "    # Drop remaining NaNs from FSM and IDACI cols\n",
    "    df = df.dropna(subset = [\"fsm\", \"idaci\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d6f60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join exam data with NPD data\n",
    "merged = npd_data.merge(gcse_data, on = [\"uidp\", \"year\"],\n",
    "                       how = \"inner\")\n",
    "\n",
    "# Recode columns and filter further\n",
    "df = recode_cols(merged)\n",
    "\n",
    "# Drop now unnecesary UIDP and year cols\n",
    "df = df.drop(columns = [\"year\", \"uidp\"])\n",
    "\n",
    "# Convert categorical cols to numerics\n",
    "categorical_cols = [\"eal\", \"gender\", \"ethnicity\", \"fsm\",\n",
    "               \"sen\", \"jcqtitle\", \"tier\", \"centretypedesc\"]\n",
    "# Fit encoder on categorical cols\n",
    "encoder = OrdinalEncoder()\n",
    "encoder.fit(df[categorical_cols])\n",
    "\n",
    "# Create a mapping for reference later\n",
    "mapping = {k:v for k, v in zip(categorical_cols, encoder.categories_)}\n",
    "\n",
    "# Convert categoricals to numerics\n",
    "df[categorical_cols] = encoder.transform(df[categorical_cols])\n",
    "\n",
    "# Split into treatment and control\n",
    "treatment = df[~df.centreassessmentgrade.isna()]\n",
    "control = df[df.centreassessmentgrade.isna()]\n",
    "# Old code, maybe useful for LGBM\n",
    "# df[categorical_cols] = df[categorical_cols].apply(pd.Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "415082a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into labels and features\n",
    "X = np.array(control.iloc[:, :10], dtype = \"float32\")\n",
    "y = control.grade.astype(\"float32\")\n",
    "\n",
    "# Split into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
    "                                                   shuffle = True,\n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e4029",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quick EDA / Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544017a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9c7f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "report = ProfileReport(df, title = \"eda_check\")\n",
    "report.to_file(\"eda_check.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc9a9d",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a906191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to store model results in\n",
    "all_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6ec31",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd767aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Generate predictions\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, train_preds, squared = False)\n",
    "test_rmse = mean_squared_error(y_test, test_preds, squared = False)\n",
    "\n",
    "# Store results\n",
    "results = pd.DataFrame({\"model\": \"linear\",\n",
    "                        \"train_rmse\": train_rmse,\n",
    "                        \"test_rmse\": test_rmse,\n",
    "             }, index = [0])\n",
    "all_results = pd.concat([all_results, results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61ce2fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.969422</td>\n",
       "      <td>1.888512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  train_rmse  test_rmse\n",
       "0  linear    1.969422   1.888512"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca950f",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "726c8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, Reshape, Activation\n",
    "from tensorflow.keras.layers import Flatten, Input\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d64afe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b27f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential(name = \"MLP\")\n",
    "\n",
    "\n",
    "model.add(Dense(hidden_dense_units, name = \"Linear_Dense\",\n",
    "                kernel_initializer = dense_kernel_initializer))\n",
    "\n",
    "# Batch normalised model\n",
    "if batch_normalize:\n",
    "    model.add(BatchNormalization(name = \"Batch_Norm1\"))\n",
    "\n",
    "# Apply non-linear activation, specified in this way to be consistent\n",
    "# with the original paper\n",
    "model.add(Activation(\"relu\", name = \"ReLU_Activation\"))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation = \"sigmoid\", name = \"Output\",\n",
    "                kernel_initializer = dense_kernel_initializer))\n",
    "# Compile model\n",
    "model.compile(loss = loss, optimizer = optimizer,\n",
    "              metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams used during modelling\n",
    "# Compilation hyperparams\n",
    "compile_hp = dict()\n",
    "compile_hp[\"loss\"] = \"binary_crossentropy\"\n",
    "compile_hp[\"optimizer\"] = optimizers.Adam(learning_rate = 0.001)\n",
    "compile_hp[\"metrics\"] = [\"accuracy\"]\n",
    "\n",
    "# Fitting hyperparams\n",
    "fit_hp = dict()\n",
    "fit_hp[\"batch_size\"] = 64\n",
    "fit_hp[\"epochs\"] = 100\n",
    "fit_hp[\"validation_split\"] = 0.3\n",
    "# Create callback to select the best model\n",
    "fit_hp[\"callbacks\"] = EarlyStopping(monitor = \"val_loss\",\n",
    "                                         mode = \"min\",\n",
    "                                         restore_best_weights = True,\n",
    "                                         patience = 25)\n",
    "\n",
    "# Eliminate verbose to have a neater notebook \n",
    "fit_hp[\"verbose\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedc019",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Compile model\n",
    "    model.compile(optimizer = opt, loss = \"mse\", metrics = [\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca67847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "history = model.fit(X_train, y_train, batch_size = batch_size, validation_split = 0.1,\n",
    "                    epochs = epochs, verbose = 0)\n",
    "\n",
    "# Show training and validation loss vs epochs\n",
    "plot_loss(history, title = \"Loss vs Epochs: 5 Latent Dimensions\")\n",
    "\n",
    "# Evalute model / show test MSE\n",
    "_, error = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print('\\033[92m\\033[1m\\n\\n         MSE: %.3f, RMSE: %.3f\\n' % (error, sqrt(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4eb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(loss = \"binary_crossentropy\",\n",
    "                optimizer = \"adam\",\n",
    "               metrics = [\"accuracy\"], regularize = False,\n",
    "               batch_normalize = False,\n",
    "               embedding = None,\n",
    "               maxlen = 500,\n",
    "               hidden_dense_units = 256,\n",
    "               dense_kernel_initializer = \"he_uniform\"):\n",
    "    \"\"\"\n",
    "    Creates an MLP designed to be used with the text data only.\n",
    "    Can be built with either Keras, SpaCy, GloVe or no embedding.\n",
    "    Returns a compiled Keras model of 3 Dense layers (1024, 256, 1). There are\n",
    "    options to include elasticnet regularization and batch normalisation layers too.\n",
    "    ----------------------------------------------------------------------\n",
    "    loss = str, name of loss function to use\n",
    "    optimizer = Keras optimizer, set to 'adam' but any optimizer can be passed\n",
    "    metrics =  list of Keras metrics to use to evaluate with\n",
    "    batch_normalize = bool, if True adds batch normalisation between hidden Dense\n",
    "    and output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build model\n",
    "    model = Sequential(name = \"MLP\")\n",
    "\n",
    "    # Add embedding if desired\n",
    "    if embedding:\n",
    "        # Embedding contains input shape\n",
    "        model.add(embedding)\n",
    "        # Flatten embeddings\n",
    "        model.add(Flatten())\n",
    "    else:\n",
    "        model.add(Input(shape = (maxlen, ), name = \"Input\"))\n",
    "\n",
    "    # Elasticnet regularised model\n",
    "    if regularize:\n",
    "        model.add(Dense(hidden_dense_units, name = \"Linear_Dense_Elasti\",\n",
    "                        kernel_regularizer = l1_l2(),\n",
    "                        kernel_initializer = dense_kernel_initializer))\n",
    "\n",
    "    # Baseline model\n",
    "    else:\n",
    "        model.add(Dense(hidden_dense_units, name = \"Linear_Dense\",\n",
    "                        kernel_initializer = dense_kernel_initializer))\n",
    "\n",
    "    # Batch normalised model\n",
    "    if batch_normalize:\n",
    "        model.add(BatchNormalization(name = \"Batch_Norm1\"))\n",
    "\n",
    "    # Apply non-linear activation, specified in this way to be consistent\n",
    "    # with the original paper\n",
    "    model.add(Activation(\"relu\", name = \"ReLU_Activation\"))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation = \"sigmoid\", name = \"Output\",\n",
    "                    kernel_initializer = dense_kernel_initializer))\n",
    "    # Compile model\n",
    "    model.compile(loss = loss, optimizer = optimizer,\n",
    "                  metrics = metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63818cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.set_seed()\n",
    "# Tweak model architecture\n",
    "model_dict[\"regularize\"] = False\n",
    "model_dict[\"batch_normalize\"] = False\n",
    "\n",
    "# Build and compile\n",
    "model = pm.mlp(regularize = model_dict[\"regularize\"],\n",
    "               batch_normalize = model_dict[\"batch_normalize\"],\n",
    "               embedding = embed_dict[model_dict[\"embedding\"]],\n",
    "               **compile_hp)\n",
    "model.summary()\n",
    "# Fit model and save it and its history\n",
    "history = pm.fit_and_save(X_train, y_train, model,\n",
    "                          save_model = save_model, \n",
    "                          save_history = save_history,\n",
    "                          **model_dict, **fit_hp)\n",
    "# Evaluate on test data\n",
    "all_results = pm.get_test_metrics(model, X_test, y_test, all_results, history, **model_dict)\n",
    "# Plot and save graphs\n",
    "pf.plot_and_save(history, save = save_plots, display = display, **model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480054da",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b69a114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea11dc6",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b6aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
